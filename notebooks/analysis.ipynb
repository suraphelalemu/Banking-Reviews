{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29067362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Banking-Reviews\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import sqlalchemy\n",
    "import oracledb\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1cdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: ReviewAnalyzer class definition\n",
    "class ReviewAnalyzer:\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tokens = [word for word in tokens if len(word) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def analyze_sentiment(self, method='textblob'):\n",
    "        \"\"\"Perform sentiment analysis\"\"\"\n",
    "        if method == 'textblob':\n",
    "            self.df['sentiment'] = self.df['clean_review'].apply(\n",
    "                lambda x: TextBlob(x).sentiment.polarity\n",
    "            )\n",
    "            self.df['sentiment_label'] = pd.cut(\n",
    "                self.df['sentiment'],\n",
    "                bins=[-1, -0.1, 0.1, 1],\n",
    "                labels=['negative', 'neutral', 'positive']\n",
    "            )\n",
    "        else:\n",
    "            # Use transformers for more accurate sentiment analysis\n",
    "            sentiment_pipeline = pipeline(\n",
    "                \"sentiment-analysis\", \n",
    "                model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "            )\n",
    "            \n",
    "            def get_sentiment(text):\n",
    "                if len(text) == 0:\n",
    "                    return 'neutral', 0\n",
    "                result = sentiment_pipeline(text[:512])[0]\n",
    "                return result['label'], result['score']\n",
    "            \n",
    "            # Apply to a sample if dataset is large\n",
    "            sample_size = min(200, len(self.df))\n",
    "            sample = self.df.sample(sample_size, random_state=42)\n",
    "            sample[['sentiment_label', 'sentiment_score']] = sample['clean_review'].apply(\n",
    "                lambda x: pd.Series(get_sentiment(x))\n",
    "            )\n",
    "            \n",
    "            # Merge back with main dataframe\n",
    "            self.df = self.df.merge(\n",
    "                sample[['sentiment_label', 'sentiment_score']],\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True\n",
    "            )\n",
    "    \n",
    "    def extract_themes(self):\n",
    "        \"\"\"Extract common themes from reviews\"\"\"\n",
    "        # Define theme categories based on keywords\n",
    "        theme_keywords = {\n",
    "            'Stability Issues': ['crash', 'bug', 'error', 'freeze', 'close'],\n",
    "            'Authentication Problems': ['login', 'password', 'otp', 'pin', 'authentic'],\n",
    "            'Transaction Issues': ['transfer', 'transaction', 'payment', 'money', 'send'],\n",
    "            'Performance': ['slow', 'fast', 'speed', 'load', 'lag'],\n",
    "            'UI/UX': ['interface', 'design', 'screen', 'button', 'layout'],\n",
    "            'Features': ['feature', 'function', 'add', 'need', 'want'],\n",
    "            'Customer Support': ['support', 'help', 'service', 'contact', 'response']\n",
    "        }\n",
    "        \n",
    "        def assign_theme(text):\n",
    "            themes = []\n",
    "            for theme, keywords in theme_keywords.items():\n",
    "                if any(keyword in text for keyword in keywords):\n",
    "                    themes.append(theme)\n",
    "            return ', '.join(themes) if themes else 'Other'\n",
    "        \n",
    "        self.df['themes'] = self.df['processed_review'].apply(assign_theme)\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Run full analysis pipeline\"\"\"\n",
    "        # Clean text\n",
    "        self.df['clean_review'] = self.df['review'].apply(self.clean_text)\n",
    "        \n",
    "        # Preprocess for thematic analysis\n",
    "        self.df['processed_review'] = self.df['clean_review'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        self.analyze_sentiment(method='textblob')  # or 'transformers'\n",
    "        \n",
    "        # Thematic analysis\n",
    "        self.extract_themes()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def visualize(self, bank_name):\n",
    "        \"\"\"Generate visualizations for a specific bank\"\"\"\n",
    "        bank_df = self.df[self.df['bank'] == bank_name]\n",
    "        \n",
    "        plt.figure(figsize=(15, 12))\n",
    "        plt.suptitle(f'{bank_name} Mobile App Review Analysis', y=1.02)\n",
    "        \n",
    "        # 1. Rating distribution\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.countplot(data=bank_df, x='rating', palette='viridis')\n",
    "        plt.title('Rating Distribution')\n",
    "        plt.xlabel('Star Rating')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # 2. Sentiment distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sentiment_counts = bank_df['sentiment_label'].value_counts()\n",
    "        sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='coolwarm')\n",
    "        plt.title('Sentiment Distribution')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # 3. Top themes\n",
    "        plt.subplot(2, 2, 3)\n",
    "        themes = bank_df['themes'].str.split(', ').explode()\n",
    "        top_themes = themes.value_counts().head(5)\n",
    "        sns.barplot(y=top_themes.index, x=top_themes.values, palette='magma')\n",
    "        plt.title('Top 5 Complaint Themes')\n",
    "        plt.xlabel('Count')\n",
    "        \n",
    "        # 4. Word cloud\n",
    "        plt.subplot(2, 2, 4)\n",
    "        text = ' '.join(bank_df['processed_review'])\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Reviews')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "         # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.abspath(os.path.join(os.getcwd(), '../output'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare output path for the visualization\n",
    "        output_path = os.path.join(output_dir, bank_name.replace(\" \", \"_\") + \"_analysis.png\")\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "#  # 4. Word cloud\n",
    "#         plt.subplot(2, 2, 4)\n",
    "#         text = ' '.join(bank_df['processed_review'])\n",
    "#         wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "#         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#         plt.axis('off')\n",
    "#         plt.title('Word Cloud of Reviews')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'{bank_name.replace(\" \", \"_\")}_analysis.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd833900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file path\n",
    "csv_file_path = \"../data/All_banks_reviews.csv\"\n",
    "\n",
    "# Instantiate the ReviewAnalyzer\n",
    "analyzer = ReviewAnalyzer(csv_file_path)\n",
    "\n",
    "try:\n",
    "    # Run full analysis pipeline\n",
    "    df_analyzed = analyzer.analyze()\n",
    "    \n",
    "    # Check if any banks are available\n",
    "    if 'bank' in df_analyzed.columns and not df_analyzed['bank'].isnull().all():\n",
    "        # Generate visualizations for each bank\n",
    "        for bank in df_analyzed['bank'].unique():\n",
    "            analyzer.visualize(bank)\n",
    "    else:\n",
    "        print(\"No bank data available for visualization.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e025466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
