{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87391a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "from google_play_scraper import app, Sort, reviews_all\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Bank apps to scrape (package names from Google Play Store URLs)\n",
    "BANK_APPS = {\n",
    "    \"Commercial Bank of Ethiopia\": \"com.combanketh.mobilebanking\",\n",
    "    \"Bank of Abyssinia\": \"com.boa.boaMobileBanking\",\n",
    "    \"Dashen Bank\": \"com.cr2.amolelight\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Review Scraper Function\n",
    "def scrape_reviews(package_name, bank_name):\n",
    "    \"\"\"\n",
    "    Scrape reviews for a specific bank app\n",
    "    \"\"\"\n",
    "    print(f\"Scraping reviews for {bank_name}...\")\n",
    "    \n",
    "    # Get app info first\n",
    "    try:\n",
    "        app_info = app(package_name)\n",
    "        print(f\"App found: {app_info['title']} ({app_info['score']} stars)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting app info: {e}\")\n",
    "        app_info = None\n",
    "    \n",
    "    # Scrape reviews\n",
    "    all_reviews = []\n",
    "    continuation_token = None\n",
    "    \n",
    "    for _ in range(5):  # 5 batches of ~100 reviews\n",
    "        try:\n",
    "            result = reviews_all(\n",
    "            package_name,\n",
    "            lang='en',\n",
    "            country='et',\n",
    "            sort=Sort.NEWEST,\n",
    "        )\n",
    "\n",
    "            all_reviews.extend(result)\n",
    "            print(f\"Collected {len(result)} reviews (total: {len(all_reviews)})\")\n",
    "            \n",
    "            if not continuation_token:\n",
    "                break\n",
    "                \n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping reviews: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_reviews)\n",
    "    \n",
    "    # Add bank name\n",
    "    df['bank'] = bank_name\n",
    "    \n",
    "    # Convert timestamp to date\n",
    "    df['date'] = pd.to_datetime(df['at']).dt.date\n",
    "    \n",
    "    # Select relevant columns\n",
    "    df = df[['content', 'score', 'date', 'bank', 'thumbsUpCount']]\n",
    "    df.columns = ['review', 'rating', 'date', 'bank', 'votes']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_reviews = []\n",
    "\n",
    "for bank_name, package_name in BANK_APPS.items():\n",
    "    try:\n",
    "        bank_reviews = scrape_reviews(package_name, bank_name)\n",
    "        all_reviews.append(bank_reviews)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {bank_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b62743",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(all_reviews, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"../data/All_banks_reviews.csv\"\n",
    "combined_df.to_csv(filename, index=False)\n",
    "print(f\"Saved {len(combined_df)} reviews to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16477db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Banking-Reviews\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\surap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import sqlalchemy\n",
    "import oracledb\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ee1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewAnalyzer:\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tokens = [word for word in tokens if len(word) > 2]\n",
    "        return ' '.join(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
